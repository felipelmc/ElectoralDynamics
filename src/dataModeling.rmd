---
title: "**Modelagem Estatística**"
subtitle: "**Modelagem dos dados**"
author: "Felipe Lamarca"
output: html_notebook
---

Vamos começar carregando os pacotes necessários para a análise.

```{r}
#library(tidyverse)
library(ggplot2)
library(dplyr)
library(readr)
library(lme4)
library(caret)
library(MuMIn)
```

Agora vamos carregar os dados que foram tratados no script de tratamento de dados.

```{r}
df <- read_csv("../data/finalData.csv")

# remove NAs from despesas_campanha
df <- df[!is.na(df$despesas_campanha), ]

# standardize despesas_campanha
df$standardized_despesas_campanha <- scale(df$despesas_campanha)
```

```{r}
ggplot(df, aes(x = standardized_despesas_campanha, y = resultado)) +
  geom_point() +                       # Plotar pontos
  geom_smooth(method = "glm",          # Adicionar linha suave ajustando com regressão logística (modelo glm)
              method.args = list(family = "binomial"),   # Especificar a família binomial para regressão logística
              se = FALSE) +            # Não mostrar intervalo de confiança
  labs(x = "Variável Independente", y = "Variável Dependente") +
  ggtitle("Relação entre Variável Independente e Variável Dependente") +
  theme_minimal()
```


```{r}
dados_train <- sample_frac(df, 0.7)  # 70% dos dados para treinamento
dados_test <- anti_join(df, dados_train)  # Dados restantes para teste
```

Nosso objetivo agora é ajustar modelos estatísticos para os dados. Em particular, suspeitamos que a variável resposta binária `resultado`
depende fortemente da variável `despesas_campanha`. Vamos ajustar um modelo de regressão logística para os dados.

```{r}
model1 <- glm(resultado ~ standardized_despesas_campanha, data = df, family = binomial)
summary(model1)
r.squaredGLMM(modelo1)
```

Vamos utilizar novamente o modelo1 para fazer predições nos dados de teste.

```{r}
# Ajustar o modelo de regressão logística
modelo1 <- glm(resultado ~ standardized_despesas_campanha,
               data = dados_train, family = binomial)

summary(modelo1)

# Fazer predições
ypred <- predict(modelo1, newdata = dados_test, type = "response")

# Converter as probabilidades em valores de classe usando um limiar de corte
ypred_class <- ifelse(ypred > 0.5, 1, 0)

# Calcular as métricas
# 1. Percentual de acerto para os casos em que o candidato foi eleito (Acurácia)
acuracia_eleito <- sum(ypred_class[dados_test$resultado == 1] == 1) / sum(dados_test$resultado == 1)

# 2. Percentual de acerto para os casos em que o candidato não foi eleito (Acurácia não eleito)
acuracia_nao_eleito <- sum(ypred_class[dados_test$resultado == 0] == 0) / sum(dados_test$resultado == 0)

# 3. Acurácia geral
acuracia_geral <- sum(ypred_class == dados_test$resultado) / length(dados_test$resultado)

# Imprimir as métricas
print(paste("Acurácia eleito:", acuracia_eleito))
print(paste("Acurácia não eleito:", acuracia_nao_eleito))
print(paste("Acurácia geral:", acuracia_geral))
pROC::auc(dados_test$resultado, ypred)
```

No caso específico dos dados eleitorais, temos a possibilidade de ajustar modelos multinível para os dados, uma vez que
os candidatos estão agrupados por estado, `sigla_uf`. Vamos ajustar um modelo multinível para os dados.

```{r}
# Fitting the logistic mixed-effects regression model with rescaled predictor
model2 <- glmer(resultado ~ standardized_despesas_campanha + (tandardized_despesas_campanha|sigla_uf), data = df, family = binomial)
summary(model2)
r.squaredGLMM(model2)
```

Parece que o modelo multinível não apresentou melhora significativa em relação ao modelo de regressão logística simples.
Inclusive, o modelo multinível apresentou um valor de AIC maior que o modelo simples.

```{r}
model3 <- glmer(resultado ~ standardized_despesas_campanha + (standardized_despesas_campanha|sigla_partido), data = df, family = binomial)
summary(model3)
r.squaredGLMM(model3)
```



```{r}
# Ajustar o modelo de regressão logística
modelo3 <- glmer(resultado ~ standardized_despesas_campanha + (standardized_despesas_campanha|sigla_partido),
                 data = dados_train, family = binomial)

summary(modelo3)

# Fazer predições
ypred <- predict(modelo3, newdata = dados_test, type = "response")

# Converter as probabilidades em valores de classe usando um limiar de corte
ypred_class <- ifelse(ypred > 0.5, 1, 0)

# Calcular as métricas
# 1. Percentual de acerto para os casos em que o candidato foi eleito (Acurácia)
acuracia_eleito <- sum(ypred_class[dados_test$resultado == 1] == 1) / sum(dados_test$resultado == 1)

# 2. Percentual de acerto para os casos em que o candidato não foi eleito (Acurácia não eleito)
acuracia_nao_eleito <- sum(ypred_class[dados_test$resultado == 0] == 0) / sum(dados_test$resultado == 0)

# 3. Acurácia geral
acuracia_geral <- sum(ypred_class == dados_test$resultado) / length(dados_test$resultado)

# Imprimir as métricas
print(paste("Acurácia eleito:", acuracia_eleito))
print(paste("Acurácia não eleito:", acuracia_nao_eleito))
print(paste("Acurácia geral:", acuracia_geral))
pROC::auc(dados_test$resultado, ypred)
```

Quando ajustamos o modelo multinível considerando o partido como agrupamento, o modelo apresentou uma melhora significativa.
O valor de AIC diminuiu em relação ao modelo simples e ao modelo multinível considerando o estado como agrupamento.

```{r}
# Ajustar o modelo de regressão logística
model4 <- glmer(resultado ~ standardized_despesas_campanha + tentando_reeleicao + (1 | sigla_partido) + (0 + tentando_reeleicao | sigla_partido),
                data = df, family = binomial)

summary(model4)
r.squaredGLMM(model4)
```

```{r}
# Ajustar o modelo de regressão logística
model4 <- glmer(resultado ~ standardized_despesas_campanha + tentando_reeleicao + (1 | sigla_partido) + (0 + tentando_reeleicao | sigla_partido),
                data = dados_train, family = binomial)

summary(model4)

# Fazer predições
ypred <- predict(model4, newdata = dados_test, type = "response")

# Converter as probabilidades em valores de classe usando um limiar de corte
ypred_class <- ifelse(ypred > 0.5, 1, 0)

# Calcular as métricas
# 1. Percentual de acerto para os casos em que o candidato foi eleito (Acurácia)
acuracia_eleito <- sum(ypred_class[dados_test$resultado == 1] == 1) / sum(dados_test$resultado == 1)

# 2. Percentual de acerto para os casos em que o candidato não foi eleito (Acurácia não eleito)
acuracia_nao_eleito <- sum(ypred_class[dados_test$resultado == 0] == 0) / sum(dados_test$resultado == 0)

# 3. Acurácia geral
acuracia_geral <- sum(ypred_class == dados_test$resultado) / nrow(dados_test)

# Imprimir as métricas
print(paste("Acurácia eleito:", acuracia_eleito))
print(paste("Acurácia não eleito:", acuracia_nao_eleito))
print(paste("Acurácia geral:", acuracia_geral))

# calculate AUC
pROC::auc(dados_test$resultado, ypred)
```

Ao que tudo indica, a despesa de campanha é um excelente preditor para o resultado de eleição, e o efeito dessa variável
varia entre os partidos. A inclusão da variável `tentando_reeleicao` melhorou significativamente o modelo, e ampliou a
capacidade preditiva no caso dos candidatos eleitos.